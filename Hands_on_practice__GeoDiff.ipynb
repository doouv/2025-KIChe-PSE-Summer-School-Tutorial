{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/doouv/2025-KIChe-PSE-Summer-School-Tutorial/blob/main/Hands_on_practice__GeoDiff.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F88mignPnalS"
      },
      "source": [
        "# **üö©Introduction**\n",
        "\n",
        "<img src=\"https://github.com/MinkaiXu/GeoDiff/raw/main/assets/geodiff_framework.png\" width=\"800px\">\n",
        "\n",
        "*   This colab is design to run the pretrained models from [GeoDiff](https://github.com/MinkaiXu/GeoDiff).\n",
        "*   The visualization code is inspired by this PyMol [colab](https://colab.research.google.com/gist/iwatobipen/2ec7faeafe5974501e69fcc98c122922/pymol.ipynb#scrollTo=Hm4kY7CaZSlw).\n",
        "\n",
        "*   The goal is to generate physically accurate molecules. Given the input of a molecule graph (atom and bond structures with their connectivity -- in the form of a 2d graph). What we want to generate is a stable 3d structure of the molecule.\n",
        "\n",
        "*   This colab uses GEOM datasets that have multiple 3d targets per configuration, which provide more compelling targets for generative methods.\n",
        "\n",
        "> [Original colab tutotial](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/geodiff_molecule_conformation.ipynb) made by [natolambert](https://twitter.com/natolambert); **this version was updated for the 2025 KiChe PSE Summer School** by TA [Hayoung Doo](https://github.com/doouv/2025-KIChe-PSE-Summer-School-Tutorial) with academic supervision from [Prof. Jonggeol Na](https://nagroup.ewha.ac.kr/).\n",
        ">\n",
        "> Library: [Hugging Face Diffusers](colab.research.google.com/drive/1oCSVfMhWrqHTeHbKgUSQN9hTKxLzoNyb?pli=1#scrollTo=QiAR1bg1D2yH)\n",
        "\n",
        "---\n",
        "\\\n",
        "<img src=\"https://media.springernature.com/full/springer-static/image/art%3A10.1038%2Fs41597-022-01288-4/MediaObjects/41597_2022_1288_Fig1_HTML.png?as=webp\" width=\"600px\">\n",
        "\n",
        "Adapted from [Axelrod, S., G√≥mez-Bombarelli, R. et al.,Sci Data 9, 185 (2022)](https://www.nature.com/articles/s41597-022-01288-4#citeas)\n",
        "\n",
        "## **üéØ Problem description**\n",
        "\n",
        "*   A single 2D chemical structure can correspond to multiple 3D conformers.\n",
        "\n",
        "*   These conformers must satisfy chemical constraints such as bond lengths, bond angles, steric repulsion, and steric hindrance.\n",
        "\n",
        "*   However, naive sampling often produces a large number of chemically invalid structures.\n",
        "\n",
        "Therefore, the goal is to learn the mapping from a 2D chemical structure to a physically valid distribution of 3D conformers, enabling generation through sampling."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üì• **Installations**\n",
        "\n"
      ],
      "metadata": {
        "id": "fvSuboMSiFi8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install Geometric Dependencies"
      ],
      "metadata": {
        "id": "ff9SxWnaNId9"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1g_6zOabItDk"
      },
      "source": [
        "Here we check the **CUDA** and **PyTorch** versions on colab. As of Aug 2025, Colab used python 3.11; install a PyTorch build compatible with python 3.11 (i.e.,‚â• 2.0)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K0ofXobG5Y-X",
        "outputId": "8c0dd87c-6831-4be7-8f3e-4982025ccc25"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2024 NVIDIA Corporation\n",
            "Built on Thu_Jun__6_02:18:23_PDT_2024\n",
            "Cuda compilation tools, release 12.5, V12.5.82\n",
            "Build cuda_12.5.r12.5/compiler.34385749_0\n"
          ]
        }
      ],
      "source": [
        "!nvcc --version"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(f\"\\nüì¢ PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "up6x3dxMfizt",
        "outputId": "0201e129-f751-40f2-ba60-6b60b92ed62f"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üì¢ PyTorch version: 2.8.0+cu126\n",
            "CUDA available: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfthW90vI0nw"
      },
      "source": [
        "Install **PyTorch Geometric (PyG)** that matches the **PyTorch + CUDA** versions printed above.\n",
        "If your versions differ, replace the wheel index at the end of the command\n",
        "(e.g., `torch-2.6.0+cu124.html`) with the one that matches your environment.\n",
        "\n",
        "+) [2025.08.20] torch and cuda version is updated to 2.8.0+cu126\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6KCV2Forl8dG",
        "outputId": "8cd4401d-ecba-4649-9673-2106376fd737"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch_geometric in /usr/local/lib/python3.12/dist-packages (2.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (3.12.15)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (2025.3.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (2.0.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (3.2.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (1.20.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch_geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->torch_geometric) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->torch_geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->torch_geometric) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->torch_geometric) (2025.8.3)\n",
            "Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.12/dist-packages (from aiosignal>=1.4.0->aiohttp->torch_geometric) (4.14.1)\n",
            "Looking in links: https://data.pyg.org/whl/torch-2.8.0+cu126.html\n",
            "Requirement already satisfied: pyg_lib in /usr/local/lib/python3.12/dist-packages (0.4.0+pt28cu126)\n",
            "Requirement already satisfied: torch_scatter in /usr/local/lib/python3.12/dist-packages (2.1.2+pt28cu126)\n",
            "Requirement already satisfied: torch_sparse in /usr/local/lib/python3.12/dist-packages (0.6.18+pt28cu126)\n",
            "Requirement already satisfied: torch_cluster in /usr/local/lib/python3.12/dist-packages (1.6.3+pt28cu126)\n",
            "Requirement already satisfied: torch_spline_conv in /usr/local/lib/python3.12/dist-packages (1.2.2+pt28cu126)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from torch_sparse) (1.16.1)\n",
            "Requirement already satisfied: numpy<2.6,>=1.25.2 in /usr/local/lib/python3.12/dist-packages (from scipy->torch_sparse) (2.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch_geometric\n",
        "!pip install pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.8.0+cu126.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppxv6Mdkalbc"
      },
      "source": [
        "### Install Diffusers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "mgQA_XN-XGY2"
      },
      "outputs": [],
      "source": [
        "# diffuers & dependencies for diffusers\n",
        "!pip install -q -U diffusers transformers  datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLE7CqlfJNUO"
      },
      "source": [
        "### Install Chemistry-specific Dependencies\n",
        "\n",
        "Install RDKit, a tool for working with and visualizing chemsitry in python (you use this to visualize the generate models later)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "0CPv_NvehRz3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28262d9a-e33a-49b6-d628-4f1171a3d7eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: rdkit in /usr/local/lib/python3.12/dist-packages (2025.3.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from rdkit) (2.0.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from rdkit) (11.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install rdkit"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **üõ†Ô∏è Create a diffusion model**"
      ],
      "metadata": {
        "id": "5ayJLVBYiTUM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model class(es)\n",
        "\n",
        "GeoDiff uses a Graph Field Network (GFN) as its backbone, wrapped within a framework called MoleculeGNN.\n",
        "MoleculeGNN contains both a global and a local branch to capture the overall atomic environment from different perspectives.\n",
        "Each branch employs a distinct type of machine learning potential models: the global branch uses [SchNet](https://arxiv.org/abs/1706.08566), while the local branch is based on the [Graph Isomorphism Network (GIN)](https://arxiv.org/pdf/1810.00826)."
      ],
      "metadata": {
        "id": "G0rMncVtNSqU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **What is Graph Field Network (GFN)?**\n",
        "*   A graph field network (GFN) is a neural network layer that takes a graph ‚Äî with nodes, edges, and 3D geometry ‚Äî and **produces a vector field over the nodes that is equivariant to rotations and translations**.\n",
        "*   It does this by predicting a scalar value for each edge, multiplying it by the unit vector along that edge, and summing over neighbors.\n",
        "*   The result is a per-node 3D vector that changes consistently when the whole molecule is rotated or translated.\n",
        "\n",
        "üîªBelow is the schematic diagram of the MoleculeGNN architecture. Let‚Äôs map each component to its corresponding part in the code!\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1C6SLpQcz5Fv6SF95XreEIwOjLZvj4GhE\" width=700>"
      ],
      "metadata": {
        "id": "d3KFC5lM6K5s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imports"
      ],
      "metadata": {
        "id": "L5FEXz5oXkzt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model adapted from GeoDiff https://github.com/MinkaiXu/GeoDiff\n",
        "# Model inspired by https://github.com/DeepGraphLearning/torchdrug/tree/master/torchdrug/models\n",
        "from dataclasses import dataclass\n",
        "from typing import Callable, Tuple, Union\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import Tensor, nn\n",
        "from torch.nn import Embedding, Linear, Module, ModuleList, Sequential\n",
        "\n",
        "from torch_geometric.nn import MessagePassing, radius, radius_graph\n",
        "from torch_geometric.typing import Adj, OptPairTensor, OptTensor, Size\n",
        "from torch_geometric.utils import dense_to_sparse, to_dense_adj\n",
        "from torch_scatter import scatter_add\n",
        "from torch_sparse import SparseTensor, coalesce\n",
        "\n",
        "from diffusers.configuration_utils import ConfigMixin, register_to_config\n",
        "from diffusers.models.modeling_utils import ModelMixin\n",
        "from diffusers.utils import BaseOutput\n"
      ],
      "metadata": {
        "id": "-3-P4w5sXkRU"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Helper classes"
      ],
      "metadata": {
        "id": "EzJQXPN_XrMX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class MoleculeGNNOutput(BaseOutput):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        sample (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n",
        "            Hidden states output. Output of last layer of model.\n",
        "    \"\"\"\n",
        "\n",
        "    sample: torch.FloatTensor\n",
        "\n",
        "\n",
        "class MultiLayerPerceptron(nn.Module):\n",
        "    \"\"\"\n",
        "    Multi-layer Perceptron. Note there is no activation or dropout in the last layer.\n",
        "    Args:\n",
        "        input_dim (int): input dimension\n",
        "        hidden_dim (list of int): hidden dimensions\n",
        "        activation (str or function, optional): activation function\n",
        "        dropout (float, optional): dropout rate\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_dim, hidden_dims, activation=\"relu\", dropout=0):\n",
        "        super(MultiLayerPerceptron, self).__init__()\n",
        "\n",
        "        self.dims = [input_dim] + hidden_dims\n",
        "        if isinstance(activation, str):\n",
        "            self.activation = getattr(F, activation)\n",
        "        else:\n",
        "            print(f\"Warning, activation passed {activation} is not string and ignored\")\n",
        "            self.activation = None\n",
        "        if dropout > 0:\n",
        "            self.dropout = nn.Dropout(dropout)\n",
        "        else:\n",
        "            self.dropout = None\n",
        "\n",
        "        self.layers = nn.ModuleList()\n",
        "        for i in range(len(self.dims) - 1):\n",
        "            self.layers.append(nn.Linear(self.dims[i], self.dims[i + 1]))\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\"\"\"\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            x = layer(x)\n",
        "            if i < len(self.layers) - 1:\n",
        "                if self.activation:\n",
        "                    x = self.activation(x)\n",
        "                if self.dropout:\n",
        "                    x = self.dropout(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ShiftedSoftplus(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ShiftedSoftplus, self).__init__()\n",
        "        self.shift = torch.log(torch.tensor(2.0)).item()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return F.softplus(x) - self.shift\n",
        "\n",
        "\n",
        "class CFConv(MessagePassing):\n",
        "    def __init__(self, in_channels, out_channels, num_filters, mlp, cutoff, smooth):\n",
        "        super(CFConv, self).__init__(aggr=\"add\")\n",
        "        self.lin1 = Linear(in_channels, num_filters, bias=False)\n",
        "        self.lin2 = Linear(num_filters, out_channels)\n",
        "        self.nn = mlp\n",
        "        self.cutoff = cutoff\n",
        "        self.smooth = smooth\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        torch.nn.init.xavier_uniform_(self.lin1.weight)\n",
        "        torch.nn.init.xavier_uniform_(self.lin2.weight)\n",
        "        self.lin2.bias.data.fill_(0)\n",
        "\n",
        "    def forward(self, x, edge_index, edge_length, edge_attr):\n",
        "        if self.smooth:\n",
        "            C = 0.5 * (torch.cos(edge_length * np.pi / self.cutoff) + 1.0)\n",
        "            C = C * (edge_length <= self.cutoff) * (edge_length >= 0.0)  # Modification: cutoff\n",
        "        else:\n",
        "            C = (edge_length <= self.cutoff).float()\n",
        "        W = self.nn(edge_attr) * C.view(-1, 1)\n",
        "\n",
        "        x = self.lin1(x)\n",
        "        x = self.propagate(edge_index, x=x, W=W)\n",
        "        x = self.lin2(x)\n",
        "        return x\n",
        "\n",
        "    def message(self, x_j: torch.Tensor, W) -> torch.Tensor:\n",
        "        return x_j * W\n",
        "\n",
        "\n",
        "class InteractionBlock(torch.nn.Module):\n",
        "    def __init__(self, hidden_channels, num_gaussians, num_filters, cutoff, smooth):\n",
        "        super(InteractionBlock, self).__init__()\n",
        "        mlp = Sequential(\n",
        "            Linear(num_gaussians, num_filters),\n",
        "            ShiftedSoftplus(),\n",
        "            Linear(num_filters, num_filters),\n",
        "        )\n",
        "        self.conv = CFConv(hidden_channels, hidden_channels, num_filters, mlp, cutoff, smooth)\n",
        "        self.act = ShiftedSoftplus()\n",
        "        self.lin = Linear(hidden_channels, hidden_channels)\n",
        "\n",
        "    def forward(self, x, edge_index, edge_length, edge_attr):\n",
        "        x = self.conv(x, edge_index, edge_length, edge_attr)\n",
        "        x = self.act(x)\n",
        "        x = self.lin(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class SchNetEncoder(Module):\n",
        "    def __init__(\n",
        "        self, hidden_channels=128, num_filters=128, num_interactions=6, edge_channels=100, cutoff=10.0, smooth=False\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.hidden_channels = hidden_channels\n",
        "        self.num_filters = num_filters\n",
        "        self.num_interactions = num_interactions\n",
        "        self.cutoff = cutoff\n",
        "\n",
        "        self.embedding = Embedding(100, hidden_channels, max_norm=10.0)\n",
        "\n",
        "        self.interactions = ModuleList()\n",
        "        for _ in range(num_interactions):\n",
        "            block = InteractionBlock(hidden_channels, edge_channels, num_filters, cutoff, smooth)\n",
        "            self.interactions.append(block)\n",
        "\n",
        "    def forward(self, z, edge_index, edge_length, edge_attr, embed_node=True):\n",
        "        if embed_node:\n",
        "            assert z.dim() == 1 and z.dtype == torch.long\n",
        "            h = self.embedding(z)\n",
        "        else:\n",
        "            h = z\n",
        "        for interaction in self.interactions:\n",
        "            h = h + interaction(h, edge_index, edge_length, edge_attr)\n",
        "\n",
        "        return h\n",
        "\n",
        "\n",
        "class GINEConv(MessagePassing):\n",
        "    \"\"\"\n",
        "    Custom class of the graph isomorphism operator from the \"How Powerful are Graph Neural Networks?\n",
        "    https://arxiv.org/abs/1810.00826 paper. Note that this implementation has the added option of a custom activation.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, mlp: Callable, eps: float = 0.0, train_eps: bool = False, activation=\"softplus\", **kwargs):\n",
        "        super(GINEConv, self).__init__(aggr=\"add\", **kwargs)\n",
        "        self.nn = mlp\n",
        "        self.initial_eps = eps\n",
        "\n",
        "        if isinstance(activation, str):\n",
        "            self.activation = getattr(F, activation)\n",
        "        else:\n",
        "            self.activation = None\n",
        "\n",
        "        if train_eps:\n",
        "            self.eps = torch.nn.Parameter(torch.Tensor([eps]))\n",
        "        else:\n",
        "            self.register_buffer(\"eps\", torch.Tensor([eps]))\n",
        "\n",
        "    def forward(\n",
        "        self, x: Union[Tensor, OptPairTensor], edge_index: Adj, edge_attr: OptTensor = None, size: Size = None\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"\"\"\"\n",
        "        if isinstance(x, torch.Tensor):\n",
        "            x: OptPairTensor = (x, x)\n",
        "\n",
        "        # Node and edge feature dimensionalites need to match.\n",
        "        if isinstance(edge_index, torch.Tensor):\n",
        "            assert edge_attr is not None\n",
        "            assert x[0].size(-1) == edge_attr.size(-1)\n",
        "        elif isinstance(edge_index, SparseTensor):\n",
        "            assert x[0].size(-1) == edge_index.size(-1)\n",
        "\n",
        "        # propagate_type: (x: OptPairTensor, edge_attr: OptTensor)\n",
        "        out = self.propagate(edge_index, x=x, edge_attr=edge_attr, size=size)\n",
        "\n",
        "        x_r = x[1]\n",
        "        if x_r is not None:\n",
        "            out += (1 + self.eps) * x_r\n",
        "\n",
        "        return self.nn(out)\n",
        "\n",
        "    def message(self, x_j: torch.Tensor, edge_attr: torch.Tensor) -> torch.Tensor:\n",
        "        if self.activation:\n",
        "            return self.activation(x_j + edge_attr)\n",
        "        else:\n",
        "            return x_j + edge_attr\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"{}(nn={})\".format(self.__class__.__name__, self.nn)\n",
        "\n",
        "\n",
        "class GINEncoder(torch.nn.Module):\n",
        "    def __init__(self, hidden_dim, num_convs=3, activation=\"relu\", short_cut=True, concat_hidden=False):\n",
        "        super().__init__()\n",
        "\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_convs = num_convs\n",
        "        self.short_cut = short_cut\n",
        "        self.concat_hidden = concat_hidden\n",
        "        self.node_emb = nn.Embedding(100, hidden_dim)\n",
        "\n",
        "        if isinstance(activation, str):\n",
        "            self.activation = getattr(F, activation)\n",
        "        else:\n",
        "            self.activation = None\n",
        "\n",
        "        self.convs = nn.ModuleList()\n",
        "        for i in range(self.num_convs):\n",
        "            self.convs.append(\n",
        "                GINEConv(\n",
        "                    MultiLayerPerceptron(hidden_dim, [hidden_dim, hidden_dim], activation=activation),\n",
        "                    activation=activation,\n",
        "                )\n",
        "            )\n",
        "\n",
        "    def forward(self, z, edge_index, edge_attr):\n",
        "        \"\"\"\n",
        "        Input:\n",
        "            data: (torch_geometric.data.Data): batched graph edge_index: bond indices of the original graph (num_node,\n",
        "            hidden) edge_attr: edge feature tensor with shape (num_edge, hidden)\n",
        "        Output:\n",
        "            node_feature: graph feature\n",
        "        \"\"\"\n",
        "\n",
        "        node_attr = self.node_emb(z)  # (num_node, hidden)\n",
        "\n",
        "        hiddens = []\n",
        "        conv_input = node_attr  # (num_node, hidden)\n",
        "\n",
        "        for conv_idx, conv in enumerate(self.convs):\n",
        "            hidden = conv(conv_input, edge_index, edge_attr)\n",
        "            if conv_idx < len(self.convs) - 1 and self.activation is not None:\n",
        "                hidden = self.activation(hidden)\n",
        "            assert hidden.shape == conv_input.shape\n",
        "            if self.short_cut and hidden.shape == conv_input.shape:\n",
        "                hidden += conv_input\n",
        "\n",
        "            hiddens.append(hidden)\n",
        "            conv_input = hidden\n",
        "\n",
        "        if self.concat_hidden:\n",
        "            node_feature = torch.cat(hiddens, dim=-1)\n",
        "        else:\n",
        "            node_feature = hiddens[-1]\n",
        "\n",
        "        return node_feature\n",
        "\n",
        "\n",
        "class MLPEdgeEncoder(Module):\n",
        "    def __init__(self, hidden_dim=100, activation=\"relu\"):\n",
        "        super().__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.bond_emb = Embedding(100, embedding_dim=self.hidden_dim)\n",
        "        self.mlp = MultiLayerPerceptron(1, [self.hidden_dim, self.hidden_dim], activation=activation)\n",
        "\n",
        "    @property\n",
        "    def out_channels(self):\n",
        "        return self.hidden_dim\n",
        "\n",
        "    def forward(self, edge_length, edge_type):\n",
        "        \"\"\"\n",
        "        Input:\n",
        "            edge_length: The length of edges, shape=(E, 1). edge_type: The type pf edges, shape=(E,)\n",
        "        Returns:\n",
        "            edge_attr: The representation of edges. (E, 2 * num_gaussians)\n",
        "        \"\"\"\n",
        "        d_emb = self.mlp(edge_length)  # (num_edge, hidden_dim)\n",
        "        edge_attr = self.bond_emb(edge_type)  # (num_edge, hidden_dim)\n",
        "        return d_emb * edge_attr  # (num_edge, hidden)\n",
        "\n",
        "\n",
        "def assemble_atom_pair_feature(node_attr, edge_index, edge_attr):\n",
        "    h_row, h_col = node_attr[edge_index[0]], node_attr[edge_index[1]]\n",
        "    h_pair = torch.cat([h_row * h_col, edge_attr], dim=-1)  # (E, 2H)\n",
        "    return h_pair\n",
        "\n",
        "\n",
        "def _extend_graph_order(num_nodes, edge_index, edge_type, order=3):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        num_nodes:  Number of atoms.\n",
        "        edge_index: Bond indices of the original graph.\n",
        "        edge_type:  Bond types of the original graph.\n",
        "        order:  Extension order.\n",
        "    Returns:\n",
        "        new_edge_index: Extended edge indices. new_edge_type: Extended edge types.\n",
        "    \"\"\"\n",
        "\n",
        "    def binarize(x):\n",
        "        return torch.where(x > 0, torch.ones_like(x), torch.zeros_like(x))\n",
        "\n",
        "    def get_higher_order_adj_matrix(adj, order):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            adj:        (N, N)\n",
        "            type_mat:   (N, N)\n",
        "        Returns:\n",
        "            Following attributes will be updated:\n",
        "              - edge_index\n",
        "              - edge_type\n",
        "            Following attributes will be added to the data object:\n",
        "              - bond_edge_index: Original edge_index.\n",
        "        \"\"\"\n",
        "        adj_mats = [\n",
        "            torch.eye(adj.size(0), dtype=torch.long, device=adj.device),\n",
        "            binarize(adj + torch.eye(adj.size(0), dtype=torch.long, device=adj.device)),\n",
        "        ]\n",
        "\n",
        "        for i in range(2, order + 1):\n",
        "            adj_mats.append(binarize(adj_mats[i - 1] @ adj_mats[1]))\n",
        "        order_mat = torch.zeros_like(adj)\n",
        "\n",
        "        for i in range(1, order + 1):\n",
        "            order_mat += (adj_mats[i] - adj_mats[i - 1]) * i\n",
        "\n",
        "        return order_mat\n",
        "\n",
        "    num_types = 22\n",
        "    # given from len(BOND_TYPES), where BOND_TYPES = {t: i for i, t in enumerate(BT.names.values())}\n",
        "    # from rdkit.Chem.rdchem import BondType as BT\n",
        "    N = num_nodes\n",
        "    adj = to_dense_adj(edge_index).squeeze(0)\n",
        "    adj_order = get_higher_order_adj_matrix(adj, order)  # (N, N)\n",
        "\n",
        "    type_mat = to_dense_adj(edge_index, edge_attr=edge_type).squeeze(0)  # (N, N)\n",
        "    type_highorder = torch.where(adj_order > 1, num_types + adj_order - 1, torch.zeros_like(adj_order))\n",
        "    assert (type_mat * type_highorder == 0).all()\n",
        "    type_new = type_mat + type_highorder\n",
        "\n",
        "    new_edge_index, new_edge_type = dense_to_sparse(type_new)\n",
        "    _, edge_order = dense_to_sparse(adj_order)\n",
        "\n",
        "    # data.bond_edge_index = data.edge_index  # Save original edges\n",
        "    new_edge_index, new_edge_type = coalesce(new_edge_index, new_edge_type.long(), N, N)  # modify data\n",
        "\n",
        "    return new_edge_index, new_edge_type\n",
        "\n",
        "\n",
        "def _extend_to_radius_graph(pos, edge_index, edge_type, cutoff, batch, unspecified_type_number=0, is_sidechain=None):\n",
        "    assert edge_type.dim() == 1\n",
        "    N = pos.size(0)\n",
        "\n",
        "    bgraph_adj = torch.sparse.LongTensor(edge_index, edge_type, torch.Size([N, N]))\n",
        "\n",
        "    if is_sidechain is None:\n",
        "        rgraph_edge_index = radius_graph(pos, r=cutoff, batch=batch)  # (2, E_r)\n",
        "    else:\n",
        "        # fetch sidechain and its batch index\n",
        "        is_sidechain = is_sidechain.bool()\n",
        "        dummy_index = torch.arange(pos.size(0), device=pos.device)\n",
        "        sidechain_pos = pos[is_sidechain]\n",
        "        sidechain_index = dummy_index[is_sidechain]\n",
        "        sidechain_batch = batch[is_sidechain]\n",
        "\n",
        "        assign_index = radius(x=pos, y=sidechain_pos, r=cutoff, batch_x=batch, batch_y=sidechain_batch)\n",
        "        r_edge_index_x = assign_index[1]\n",
        "        r_edge_index_y = assign_index[0]\n",
        "        r_edge_index_y = sidechain_index[r_edge_index_y]\n",
        "\n",
        "        rgraph_edge_index1 = torch.stack((r_edge_index_x, r_edge_index_y))  # (2, E)\n",
        "        rgraph_edge_index2 = torch.stack((r_edge_index_y, r_edge_index_x))  # (2, E)\n",
        "        rgraph_edge_index = torch.cat((rgraph_edge_index1, rgraph_edge_index2), dim=-1)  # (2, 2E)\n",
        "        # delete self loop\n",
        "        rgraph_edge_index = rgraph_edge_index[:, (rgraph_edge_index[0] != rgraph_edge_index[1])]\n",
        "\n",
        "    rgraph_adj = torch.sparse.LongTensor(\n",
        "        rgraph_edge_index,\n",
        "        torch.ones(rgraph_edge_index.size(1)).long().to(pos.device) * unspecified_type_number,\n",
        "        torch.Size([N, N]),\n",
        "    )\n",
        "\n",
        "    composed_adj = (bgraph_adj + rgraph_adj).coalesce()  # Sparse (N, N, T)\n",
        "\n",
        "    new_edge_index = composed_adj.indices()\n",
        "    new_edge_type = composed_adj.values().long()\n",
        "\n",
        "    return new_edge_index, new_edge_type\n",
        "\n",
        "\n",
        "def extend_graph_order_radius(\n",
        "    num_nodes,\n",
        "    pos,\n",
        "    edge_index,\n",
        "    edge_type,\n",
        "    batch,\n",
        "    order=3,\n",
        "    cutoff=10.0,\n",
        "    extend_order=True,\n",
        "    extend_radius=True,\n",
        "    is_sidechain=None,\n",
        "):\n",
        "    if extend_order:\n",
        "        edge_index, edge_type = _extend_graph_order(\n",
        "            num_nodes=num_nodes, edge_index=edge_index, edge_type=edge_type, order=order\n",
        "        )\n",
        "\n",
        "    if extend_radius:\n",
        "        edge_index, edge_type = _extend_to_radius_graph(\n",
        "            pos=pos, edge_index=edge_index, edge_type=edge_type, cutoff=cutoff, batch=batch, is_sidechain=is_sidechain\n",
        "        )\n",
        "\n",
        "    return edge_index, edge_type\n",
        "\n",
        "\n",
        "def get_distance(pos, edge_index):\n",
        "    return (pos[edge_index[0]] - pos[edge_index[1]]).norm(dim=-1)\n",
        "\n",
        "\n",
        "def graph_field_network(score_d, pos, edge_index, edge_length):\n",
        "    \"\"\"\n",
        "    Transformation to make the epsilon predicted from the diffusion model roto-translational equivariant. See equations\n",
        "    5-7 of the GeoDiff Paper https://arxiv.org/pdf/2203.02923.pdf\n",
        "    \"\"\"\n",
        "    N = pos.size(0)\n",
        "    dd_dr = (1.0 / edge_length) * (pos[edge_index[0]] - pos[edge_index[1]])  # (E, 3)\n",
        "    score_pos = scatter_add(dd_dr * score_d, edge_index[0], dim=0, dim_size=N) + scatter_add(\n",
        "        -dd_dr * score_d, edge_index[1], dim=0, dim_size=N\n",
        "    )  # (N, 3)\n",
        "    return score_pos\n",
        "\n",
        "\n",
        "def clip_norm(vec, limit, p=2):\n",
        "    norm = torch.norm(vec, dim=-1, p=2, keepdim=True)\n",
        "    denom = torch.where(norm > limit, limit / norm, torch.ones_like(norm))\n",
        "    return vec * denom\n",
        "\n",
        "\n",
        "def is_local_edge(edge_type):\n",
        "    return edge_type > 0\n"
      ],
      "metadata": {
        "id": "oR1Y56QiLY90"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Main model class!"
      ],
      "metadata": {
        "id": "QWrHJFcYXyUB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MoleculeGNN(ModelMixin, ConfigMixin):\n",
        "    @register_to_config\n",
        "    def __init__(\n",
        "        self,\n",
        "        hidden_dim=128,\n",
        "        num_convs=6,\n",
        "        num_convs_local=4,\n",
        "        cutoff=10.0,\n",
        "        mlp_act=\"relu\",\n",
        "        edge_order=3,\n",
        "        edge_encoder=\"mlp\",\n",
        "        smooth_conv=True,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.cutoff = cutoff\n",
        "        self.edge_encoder = edge_encoder\n",
        "        self.edge_order = edge_order\n",
        "\n",
        "        \"\"\"\n",
        "        edge_encoder: Takes both edge type and edge length as input and outputs a vector [Note]: node embedding is done\n",
        "        in SchNetEncoder\n",
        "        \"\"\"\n",
        "        self.edge_encoder_global = MLPEdgeEncoder(hidden_dim, mlp_act)  # get_edge_encoder(config)\n",
        "        self.edge_encoder_local = MLPEdgeEncoder(hidden_dim, mlp_act)  # get_edge_encoder(config)\n",
        "\n",
        "        \"\"\"\n",
        "        The graph neural network that extracts node-wise features.\n",
        "        \"\"\"\n",
        "        self.encoder_global = SchNetEncoder(\n",
        "            hidden_channels=hidden_dim,\n",
        "            num_filters=hidden_dim,\n",
        "            num_interactions=num_convs,\n",
        "            edge_channels=self.edge_encoder_global.out_channels,\n",
        "            cutoff=cutoff,\n",
        "            smooth=smooth_conv,\n",
        "        )\n",
        "        self.encoder_local = GINEncoder(\n",
        "            hidden_dim=hidden_dim,\n",
        "            num_convs=num_convs_local,\n",
        "        )\n",
        "\n",
        "        \"\"\"\n",
        "        `output_mlp` takes a mixture of two nodewise features and edge features as input and outputs\n",
        "            gradients w.r.t. edge_length (out_dim = 1).\n",
        "        \"\"\"\n",
        "        self.grad_global_dist_mlp = MultiLayerPerceptron(\n",
        "            2 * hidden_dim, [hidden_dim, hidden_dim // 2, 1], activation=mlp_act\n",
        "        )\n",
        "\n",
        "        self.grad_local_dist_mlp = MultiLayerPerceptron(\n",
        "            2 * hidden_dim, [hidden_dim, hidden_dim // 2, 1], activation=mlp_act\n",
        "        )\n",
        "\n",
        "        \"\"\"\n",
        "        Incorporate parameters together\n",
        "        \"\"\"\n",
        "        self.model_global = nn.ModuleList([self.edge_encoder_global, self.encoder_global, self.grad_global_dist_mlp])\n",
        "        self.model_local = nn.ModuleList([self.edge_encoder_local, self.encoder_local, self.grad_local_dist_mlp])\n",
        "\n",
        "    def _forward(\n",
        "        self,\n",
        "        atom_type,\n",
        "        pos,\n",
        "        bond_index,\n",
        "        bond_type,\n",
        "        batch,\n",
        "        time_step,  # NOTE, model trained without timestep performed best\n",
        "        edge_index=None,\n",
        "        edge_type=None,\n",
        "        edge_length=None,\n",
        "        return_edges=False,\n",
        "        extend_order=True,\n",
        "        extend_radius=True,\n",
        "        is_sidechain=None,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            atom_type:  Types of atoms, (N, ).\n",
        "            bond_index: Indices of bonds (not extended, not radius-graph), (2, E).\n",
        "            bond_type:  Bond types, (E, ).\n",
        "            batch:      Node index to graph index, (N, ).\n",
        "        \"\"\"\n",
        "        N = atom_type.size(0)\n",
        "        if edge_index is None or edge_type is None or edge_length is None:\n",
        "            edge_index, edge_type = extend_graph_order_radius(\n",
        "                num_nodes=N,\n",
        "                pos=pos,\n",
        "                edge_index=bond_index,\n",
        "                edge_type=bond_type,\n",
        "                batch=batch,\n",
        "                order=self.edge_order,\n",
        "                cutoff=self.cutoff,\n",
        "                extend_order=extend_order,\n",
        "                extend_radius=extend_radius,\n",
        "                is_sidechain=is_sidechain,\n",
        "            )\n",
        "            edge_length = get_distance(pos, edge_index).unsqueeze(-1)  # (E, 1)\n",
        "        local_edge_mask = is_local_edge(edge_type)  # (E, )\n",
        "\n",
        "        # with the parameterization of NCSNv2\n",
        "        # DDPM loss implicit handle the noise variance scale conditioning\n",
        "        sigma_edge = torch.ones(size=(edge_index.size(1), 1), device=pos.device)  # (E, 1)\n",
        "\n",
        "        # Encoding global\n",
        "        edge_attr_global = self.edge_encoder_global(edge_length=edge_length, edge_type=edge_type)  # Embed edges\n",
        "\n",
        "        # Global\n",
        "        node_attr_global = self.encoder_global(\n",
        "            z=atom_type,\n",
        "            edge_index=edge_index,\n",
        "            edge_length=edge_length,\n",
        "            edge_attr=edge_attr_global,\n",
        "        )\n",
        "        # Assemble pairwise features\n",
        "        h_pair_global = assemble_atom_pair_feature(\n",
        "            node_attr=node_attr_global,\n",
        "            edge_index=edge_index,\n",
        "            edge_attr=edge_attr_global,\n",
        "        )  # (E_global, 2H)\n",
        "        # Invariant features of edges (radius graph, global)\n",
        "        edge_inv_global = self.grad_global_dist_mlp(h_pair_global) * (1.0 / sigma_edge)  # (E_global, 1)\n",
        "\n",
        "        # Encoding local\n",
        "        edge_attr_local = self.edge_encoder_global(edge_length=edge_length, edge_type=edge_type)  # Embed edges\n",
        "        # edge_attr += temb_edge\n",
        "\n",
        "        # Local\n",
        "        node_attr_local = self.encoder_local(\n",
        "            z=atom_type,\n",
        "            edge_index=edge_index[:, local_edge_mask],\n",
        "            edge_attr=edge_attr_local[local_edge_mask],\n",
        "        )\n",
        "        # Assemble pairwise features\n",
        "        h_pair_local = assemble_atom_pair_feature(\n",
        "            node_attr=node_attr_local,\n",
        "            edge_index=edge_index[:, local_edge_mask],\n",
        "            edge_attr=edge_attr_local[local_edge_mask],\n",
        "        )  # (E_local, 2H)\n",
        "\n",
        "        # Invariant features of edges (bond graph, local)\n",
        "        if isinstance(sigma_edge, torch.Tensor):\n",
        "            edge_inv_local = self.grad_local_dist_mlp(h_pair_local) * (\n",
        "                1.0 / sigma_edge[local_edge_mask]\n",
        "            )  # (E_local, 1)\n",
        "        else:\n",
        "            edge_inv_local = self.grad_local_dist_mlp(h_pair_local) * (1.0 / sigma_edge)  # (E_local, 1)\n",
        "\n",
        "        if return_edges:\n",
        "            return edge_inv_global, edge_inv_local, edge_index, edge_type, edge_length, local_edge_mask\n",
        "        else:\n",
        "            return edge_inv_global, edge_inv_local\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        sample,\n",
        "        timestep: Union[torch.Tensor, float, int],\n",
        "        return_dict: bool = True,\n",
        "        sigma=1.0,\n",
        "        global_start_sigma=0.5,\n",
        "        w_global=1.0,\n",
        "        extend_order=False,\n",
        "        extend_radius=True,\n",
        "        clip_local=None,\n",
        "        clip_global=1000.0,\n",
        "    ) -> Union[MoleculeGNNOutput, Tuple]:\n",
        "        r\"\"\"\n",
        "        Args:\n",
        "            sample: packed torch geometric object\n",
        "            timestep (`torch.FloatTensor` or `float` or `int): TODO verify type and shape (batch) timesteps\n",
        "            return_dict (`bool`, *optional*, defaults to `True`):\n",
        "                Whether or not to return a [`~models.molecule_gnn.MoleculeGNNOutput`] instead of a plain tuple.\n",
        "        Returns:\n",
        "            [`~models.molecule_gnn.MoleculeGNNOutput`] or `tuple`: [`~models.molecule_gnn.MoleculeGNNOutput`] if\n",
        "            `return_dict` is True, otherwise a `tuple`. When returning a tuple, the first element is the sample tensor.\n",
        "        \"\"\"\n",
        "\n",
        "        # unpack sample\n",
        "        atom_type = sample.atom_type\n",
        "        bond_index = sample.edge_index\n",
        "        bond_type = sample.edge_type\n",
        "        num_graphs = sample.num_graphs\n",
        "        pos = sample.pos\n",
        "\n",
        "        timesteps = torch.full(size=(num_graphs,), fill_value=timestep, dtype=torch.long, device=pos.device)\n",
        "\n",
        "        edge_inv_global, edge_inv_local, edge_index, edge_type, edge_length, local_edge_mask = self._forward(\n",
        "            atom_type=atom_type,\n",
        "            pos=sample.pos,\n",
        "            bond_index=bond_index,\n",
        "            bond_type=bond_type,\n",
        "            batch=sample.batch,\n",
        "            time_step=timesteps,\n",
        "            return_edges=True,\n",
        "            extend_order=extend_order,\n",
        "            extend_radius=extend_radius,\n",
        "        )  # (E_global, 1), (E_local, 1)\n",
        "\n",
        "        # Important equation in the paper for equivariant features - eqns 5-7 of GeoDiff\n",
        "        node_eq_local = graph_field_network(\n",
        "            edge_inv_local, pos, edge_index[:, local_edge_mask], edge_length[local_edge_mask]\n",
        "        )\n",
        "        if clip_local is not None:\n",
        "            node_eq_local = clip_norm(node_eq_local, limit=clip_local)\n",
        "\n",
        "        # Global\n",
        "        if sigma < global_start_sigma:\n",
        "            edge_inv_global = edge_inv_global * (1 - local_edge_mask.view(-1, 1).float())\n",
        "            node_eq_global = graph_field_network(edge_inv_global, pos, edge_index, edge_length)\n",
        "            node_eq_global = clip_norm(node_eq_global, limit=clip_global)\n",
        "        else:\n",
        "            node_eq_global = 0\n",
        "\n",
        "        # Sum\n",
        "        eps_pos = node_eq_local + node_eq_global * w_global\n",
        "\n",
        "        if not return_dict:\n",
        "            return (-eps_pos,)\n",
        "\n",
        "        return MoleculeGNNOutput(sample=torch.FloatTensor(-eps_pos).to(pos.device))"
      ],
      "metadata": {
        "id": "MCeZA1qQXzoK"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CCIrPYSJj9wd"
      },
      "source": [
        "### Load pretrained model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YdrAr6Ch--Ab"
      },
      "source": [
        "#### Load a model\n",
        "The model used is a design an\n",
        "equivariant convolutional layer, named graph field network (GFN).\n",
        "\n",
        "The warning about `betas` and `alphas` can be ignored, those were moved to the scheduler."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "DyCo0nsqjbml",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66c982a9-d308-431a-a875-3419d6b821b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "An error occurred while trying to fetch fusing/gfn-molecule-gen-drugs: fusing/gfn-molecule-gen-drugs does not appear to have a file named diffusion_pytorch_model.safetensors.\n",
            "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n",
            "The config attributes {'type': 'diffusion', 'network': 'dualenc', 'beta_schedule': 'sigmoid', 'beta_start': 1e-07, 'beta_end': 0.002, 'num_diffusion_timesteps': 5000} were passed to MoleculeGNN, but are not expected and will be ignored. Please verify your config.json configuration file.\n",
            "Some weights of the model checkpoint at fusing/gfn-molecule-gen-drugs were not used when initializing MoleculeGNN: \n",
            " ['betas, alphas']\n"
          ]
        }
      ],
      "source": [
        "DEVICE = 'cuda'\n",
        "model = MoleculeGNN.from_pretrained(\"fusing/gfn-molecule-gen-drugs\").to(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nLRyLfOQRSV6",
        "outputId": "88bab2be-fb80-4f94-e1c4-293e5b13530d"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MoleculeGNN(\n",
            "  (edge_encoder_global): MLPEdgeEncoder(\n",
            "    (bond_emb): Embedding(100, 128)\n",
            "    (mlp): MultiLayerPerceptron(\n",
            "      (layers): ModuleList(\n",
            "        (0): Linear(in_features=1, out_features=128, bias=True)\n",
            "        (1): Linear(in_features=128, out_features=128, bias=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (edge_encoder_local): MLPEdgeEncoder(\n",
            "    (bond_emb): Embedding(100, 128)\n",
            "    (mlp): MultiLayerPerceptron(\n",
            "      (layers): ModuleList(\n",
            "        (0): Linear(in_features=1, out_features=128, bias=True)\n",
            "        (1): Linear(in_features=128, out_features=128, bias=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (encoder_global): SchNetEncoder(\n",
            "    (embedding): Embedding(100, 128, max_norm=10.0)\n",
            "    (interactions): ModuleList(\n",
            "      (0-5): 6 x InteractionBlock(\n",
            "        (conv): CFConv()\n",
            "        (act): ShiftedSoftplus()\n",
            "        (lin): Linear(in_features=128, out_features=128, bias=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (encoder_local): GINEncoder(\n",
            "    (node_emb): Embedding(100, 128)\n",
            "    (convs): ModuleList(\n",
            "      (0-3): 4 x GINEConv(nn=MultiLayerPerceptron(\n",
            "        (layers): ModuleList(\n",
            "          (0-1): 2 x Linear(in_features=128, out_features=128, bias=True)\n",
            "        )\n",
            "      ))\n",
            "    )\n",
            "  )\n",
            "  (grad_global_dist_mlp): MultiLayerPerceptron(\n",
            "    (layers): ModuleList(\n",
            "      (0): Linear(in_features=256, out_features=128, bias=True)\n",
            "      (1): Linear(in_features=128, out_features=64, bias=True)\n",
            "      (2): Linear(in_features=64, out_features=1, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (grad_local_dist_mlp): MultiLayerPerceptron(\n",
            "    (layers): ModuleList(\n",
            "      (0): Linear(in_features=256, out_features=128, bias=True)\n",
            "      (1): Linear(in_features=128, out_features=64, bias=True)\n",
            "      (2): Linear(in_features=64, out_features=1, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (model_global): ModuleList(\n",
            "    (0): MLPEdgeEncoder(\n",
            "      (bond_emb): Embedding(100, 128)\n",
            "      (mlp): MultiLayerPerceptron(\n",
            "        (layers): ModuleList(\n",
            "          (0): Linear(in_features=1, out_features=128, bias=True)\n",
            "          (1): Linear(in_features=128, out_features=128, bias=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (1): SchNetEncoder(\n",
            "      (embedding): Embedding(100, 128, max_norm=10.0)\n",
            "      (interactions): ModuleList(\n",
            "        (0-5): 6 x InteractionBlock(\n",
            "          (conv): CFConv()\n",
            "          (act): ShiftedSoftplus()\n",
            "          (lin): Linear(in_features=128, out_features=128, bias=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (2): MultiLayerPerceptron(\n",
            "      (layers): ModuleList(\n",
            "        (0): Linear(in_features=256, out_features=128, bias=True)\n",
            "        (1): Linear(in_features=128, out_features=64, bias=True)\n",
            "        (2): Linear(in_features=64, out_features=1, bias=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (model_local): ModuleList(\n",
            "    (0): MLPEdgeEncoder(\n",
            "      (bond_emb): Embedding(100, 128)\n",
            "      (mlp): MultiLayerPerceptron(\n",
            "        (layers): ModuleList(\n",
            "          (0): Linear(in_features=1, out_features=128, bias=True)\n",
            "          (1): Linear(in_features=128, out_features=128, bias=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (1): GINEncoder(\n",
            "      (node_emb): Embedding(100, 128)\n",
            "      (convs): ModuleList(\n",
            "        (0-3): 4 x GINEConv(nn=MultiLayerPerceptron(\n",
            "          (layers): ModuleList(\n",
            "            (0-1): 2 x Linear(in_features=128, out_features=128, bias=True)\n",
            "          )\n",
            "        ))\n",
            "      )\n",
            "    )\n",
            "    (2): MultiLayerPerceptron(\n",
            "      (layers): ModuleList(\n",
            "        (0): Linear(in_features=256, out_features=128, bias=True)\n",
            "        (1): Linear(in_features=128, out_features=64, bias=True)\n",
            "        (2): Linear(in_features=64, out_features=1, bias=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The warnings above are because the pre-trained model was uploaded before cleaning the code!"
      ],
      "metadata": {
        "id": "HdclRaqoUWUD"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PlOkPySoJ1m9"
      },
      "source": [
        "#### Create scheduler\n",
        "Note, other schedulers are used in the paper for slightly improved performance over DDPM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "nNHnIk9CkAb2"
      },
      "outputs": [],
      "source": [
        "from diffusers import DDPMScheduler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "RnDJdDBztjFF"
      },
      "outputs": [],
      "source": [
        "num_timesteps = 1000\n",
        "scheduler = DDPMScheduler(num_train_timesteps=num_timesteps,beta_schedule=\"sigmoid\",beta_start=1e-7, beta_end=2e-3, clip_sample=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1vh3fpSAflkL"
      },
      "source": [
        "### Get a dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KUNxfK3ln98Q"
      },
      "source": [
        "Load the dataset with torch."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, pickle\n",
        "from rdkit import Chem\n",
        "import networkx as nx\n",
        "torch.serialization.add_safe_globals([Chem.Mol, Chem.rdchem.Mol, nx.Graph, nx.DiGraph]) # For safe-load"
      ],
      "metadata": {
        "id": "ZptVb6timLKn"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q https://huggingface.co/datasets/fusing/geodiff-example-data/resolve/main/data/molecules.pkl -O molecules.pkl\n",
        "dataset = torch.load(\"molecules.pkl\")"
      ],
      "metadata": {
        "id": "KlrjsSyKmOqn"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since the `dataset` object is a legacy PyG dataset, it needs to be migrated to the current API."
      ],
      "metadata": {
        "id": "EeWbwP4Pmzvy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.data import Data\n",
        "\n",
        "def convert_old_pyg_to_new(old_data):\n",
        "    if isinstance(old_data, Data) and hasattr(old_data.__dict__, '_store'):\n",
        "        return old_data\n",
        "\n",
        "    new_data = Data()\n",
        "\n",
        "    old_dict = getattr(old_data, '__dict__', {})\n",
        "    for key, value in old_dict.items():\n",
        "        if not key.startswith('_') and value is not None:\n",
        "            setattr(new_data, key, value)\n",
        "\n",
        "    # num_nodes inference\n",
        "    if not hasattr(new_data, 'num_nodes') or new_data.num_nodes is None:\n",
        "        # confim in an order of pos_ref-> x-> pos\n",
        "        for attr in ['pos_ref', 'x', 'pos']:\n",
        "            if hasattr(new_data, attr):\n",
        "                tensor = getattr(new_data, attr)\n",
        "                if tensor is not None:\n",
        "                    new_data.num_nodes = int(tensor.size(0))\n",
        "                    break\n",
        "\n",
        "    return new_data"
      ],
      "metadata": {
        "id": "pbtShAbYnAs-"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = [convert_old_pyg_to_new(data) for data in dataset]"
      ],
      "metadata": {
        "id": "OQ1pfMEonSkn"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZcmy1EvKQRk"
      },
      "source": [
        "Print out one entry of the dataset, it contains molecular formulas, atom types, positions, and more."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "JVjz6iH_H6Eh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca5c3914-0ab3-4e2e-9a7c-d5588a3e3ff0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total molecules: 5\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Data(edge_index=[2, 598], pos=[51, 3], atom_type=[51], edge_type=[598], rdmol=<rdkit.Chem.rdchem.Mol object at 0x7a5b683c9d00>, smiles='CC1CCCN(C(=O)C2CCN(S(=O)(=O)c3cccc4nonc34)CC2)C1', nx=Graph with 51 nodes and 54 edges, idx=[1], pos_ref=[255, 3], num_pos_ref=[1], num_nodes_per_graph=[1], bond_edge_index=[2, 108], edge_order=[598], is_bond=[598])"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ],
      "source": [
        "print(f\"Total molecules: {len(dataset)}\")\n",
        "dataset[0]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset[0]['pos']"
      ],
      "metadata": {
        "id": "dsAmeTVW-UyZ"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **üß† Run the diffusion process**"
      ],
      "metadata": {
        "id": "vHNiZAUxNgoy"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZ1KZrxKqENg"
      },
      "source": [
        "#### Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "s240tYueqKKf"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.data import Data, Batch\n",
        "from torch_scatter import scatter_add, scatter_mean\n",
        "from tqdm import tqdm\n",
        "import copy\n",
        "import os\n",
        "\n",
        "def repeat_data(data: Data, num_repeat) -> Batch:\n",
        "    datas = [copy.deepcopy(data) for i in range(num_repeat)]\n",
        "    return Batch.from_data_list(datas)\n",
        "\n",
        "def repeat_batch(batch: Batch, num_repeat) -> Batch:\n",
        "    datas = batch.to_data_list()\n",
        "    new_data = []\n",
        "    for i in range(num_repeat):\n",
        "        new_data += copy.deepcopy(datas)\n",
        "    return Batch.from_data_list(new_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMnQTk0eqT7Z"
      },
      "source": [
        "#### Constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "WYGkzqgzrHmF"
      },
      "outputs": [],
      "source": [
        "num_samples = 1 # solutions per molecule\n",
        "num_molecules = 3\n",
        "\n",
        "DEVICE = 'cuda'\n",
        "sampling_type = 'ddpm_noisy' #'' # paper also uses \"generalize\" and \"ld\"\n",
        "# constants for inference\n",
        "w_global = 0.5 #0,.3 for qm9\n",
        "global_start_sigma = 0.5\n",
        "eta = 1.0\n",
        "clip_local = None\n",
        "clip_pos = None\n",
        "\n",
        "# constands for data handling\n",
        "save_traj = False\n",
        "save_data = False\n",
        "output_dir = '/content/'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xD5bJ3SqM7t"
      },
      "source": [
        "#### Generate samples!\n",
        "Note that the 3D representation of a molecule is referred to as the **conformation**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = []\n",
        "\n",
        "# define sigmas\n",
        "sigmas = torch.tensor(1.0 - scheduler.alphas_cumprod).sqrt() / torch.tensor(scheduler.alphas_cumprod).sqrt()\n",
        "sigmas = sigmas.to(DEVICE)\n",
        "\n",
        "for count, data in enumerate(tqdm(dataset)):\n",
        "    num_samples = max(data.pos_ref.size(0) // data.num_nodes, 1)\n",
        "\n",
        "    data_input = data.clone()\n",
        "    data_input['pos_ref'] = None\n",
        "    batch = repeat_data(data_input, num_samples).to(DEVICE)\n",
        "\n",
        "    # initial configuration\n",
        "    pos_init = torch.randn(batch.num_nodes, 3).to(DEVICE)\n",
        "\n",
        "    # for logging animation of denoising\n",
        "    pos_traj = []\n",
        "    with torch.no_grad():\n",
        "\n",
        "        # scale initial sample\n",
        "        pos = pos_init * sigmas[-1]\n",
        "        for t in scheduler.timesteps:\n",
        "            batch.pos = pos\n",
        "\n",
        "            # generate geometry with model, then filter it\n",
        "            epsilon = model.forward(batch, t, sigma=sigmas[t], return_dict=False)[0]\n",
        "\n",
        "            # Update\n",
        "            reconstructed_pos = scheduler.step(epsilon, t, pos)[\"prev_sample\"].to(DEVICE)\n",
        "\n",
        "            pos = reconstructed_pos\n",
        "\n",
        "            if torch.isnan(pos).any():\n",
        "                print(\"NaN detected. Please restart.\")\n",
        "                raise FloatingPointError()\n",
        "\n",
        "            # recenter graph of positions for next iteration\n",
        "            pos = pos - scatter_mean(pos, batch.batch, dim=0)[batch.batch]\n",
        "\n",
        "            # optional clipping\n",
        "            if clip_pos is not None:\n",
        "                pos = torch.clamp(pos, min=-clip_pos, max=clip_pos)\n",
        "            pos_traj.append(pos.clone().cpu())\n",
        "\n",
        "    pos_gen = pos.cpu()\n",
        "    if save_traj:\n",
        "        pos_gen_traj = pos_traj.cpu()\n",
        "        data.pos_gen = torch.stack(pos_gen_traj)\n",
        "    else:\n",
        "        data.pos_gen = pos_gen\n",
        "    results.append(data)\n",
        "\n",
        "\n",
        "if save_data:\n",
        "  save_path = os.path.join(output_dir, 'samples_all.pkl')\n",
        "\n",
        "  with open(save_path, 'wb') as f:\n",
        "      pickle.dump(results, f)"
      ],
      "metadata": {
        "id": "5lsjVUz_n6Nr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eab0cb57-7b9d-42e4-df5f-e3580d19da7e"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2015557481.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  sigmas = torch.tensor(1.0 - scheduler.alphas_cumprod).sqrt() / torch.tensor(scheduler.alphas_cumprod).sqrt()\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:57<00:00, 11.43s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Support for third party widgets will remain active for the duration of the session. To disable support:"
      ],
      "metadata": {
        "id": "TnZADgBKrMeJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **üëÅÔ∏è Render the results!**"
      ],
      "metadata": {
        "id": "grgjJFKih-tE"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d47Zxo2OKdgZ"
      },
      "source": [
        "This function allows us to render 3d in colab."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Helper functions"
      ],
      "metadata": {
        "id": "RjaVuR15NqzF"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28rBYa9NKhlz"
      },
      "source": [
        "Here is a helper function for copying the generated tensors into a format used by RDKit & NGLViewer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "LKdKdwxcyTQ6"
      },
      "outputs": [],
      "source": [
        "from copy import deepcopy\n",
        "def set_rdmol_positions(rdkit_mol, pos):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        rdkit_mol:  An `rdkit.Chem.rdchem.Mol` object.\n",
        "        pos: (N_atoms, 3)\n",
        "    \"\"\"\n",
        "    mol = deepcopy(rdkit_mol)\n",
        "    set_rdmol_positions_(mol, pos)\n",
        "    return mol\n",
        "\n",
        "def set_rdmol_positions_(mol, pos):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        rdkit_mol:  An `rdkit.Chem.rdchem.Mol` object.\n",
        "        pos: (N_atoms, 3)\n",
        "    \"\"\"\n",
        "    for i in range(pos.shape[0]):\n",
        "        mol.GetConformer(0).SetAtomPosition(i, pos[i].tolist())\n",
        "    return mol\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NuE10hcpKmzK"
      },
      "source": [
        "Process the generated data to make it easy to view."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "KieVE1vc0_Vs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cda071e2-0e0b-43b7-ffcd-bc2f587daa18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "collect 5 generated molecules in `mols`\n"
          ]
        }
      ],
      "source": [
        "# the model can generate multiple conformations per 2d geometry\n",
        "num_gen = results[0]['pos_gen'].shape[0]\n",
        "\n",
        "# init storage objects\n",
        "mols_gen = []\n",
        "mols_orig = []\n",
        "for to_process in results:\n",
        "\n",
        "    # store the reference 3d position\n",
        "    to_process['pos_ref'] = to_process['pos_ref'].reshape(-1, to_process['rdmol'].GetNumAtoms(), 3)\n",
        "\n",
        "    # store the generated 3d position\n",
        "    to_process['pos_gen'] = to_process['pos_gen'].reshape(-1, to_process['rdmol'].GetNumAtoms(), 3)\n",
        "\n",
        "    # copy data to new object\n",
        "    new_mol = set_rdmol_positions(to_process.rdmol, to_process['pos_gen'][0])\n",
        "\n",
        "    # append results\n",
        "    mols_gen.append(new_mol)\n",
        "    mols_orig.append(to_process.rdmol)\n",
        "\n",
        "print(f\"collect {len(mols_gen)} generated molecules in `mols`\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tin89JwMKp4v"
      },
      "source": [
        "Import tools to visualize the 2d chemical diagram of the molecule."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "yqV6gllSZn38"
      },
      "outputs": [],
      "source": [
        "from rdkit.Chem import AllChem\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem.Draw import rdMolDraw2D as MD2\n",
        "from IPython.display import SVG, display"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Viewing"
      ],
      "metadata": {
        "id": "hkb8w0_SNtU8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "üìå Select a molecule (0‚Äì4) by setting `idx`."
      ],
      "metadata": {
        "id": "RzCWtf92bG6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "idx = 2 # int: 0-4\n",
        "assert idx < len(results), \"selected molecule that was not generated\""
      ],
      "metadata": {
        "id": "3MZPKQUpudJ1"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, check out the coordinates of the molecule."
      ],
      "metadata": {
        "id": "0rX2kb66vAX5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Chem.MolToXYZFile(mols_gen[idx],'test.xyz')\n",
        "xyz_string = Chem.MolToXYZBlock(mols_gen[idx])\n",
        "print(xyz_string)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qFzG9sNAu6ZZ",
        "outputId": "1dfac746-e52a-4932-ce99-ded2c96d2ac7"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "41\n",
            "\n",
            "C     -0.447951    0.105039   -2.442769\n",
            "C      0.854655    0.921963   -2.340018\n",
            "O      1.083520    1.336908   -0.978605\n",
            "C      0.590669    2.620887   -0.604237\n",
            "C      1.095609    3.659087   -1.056251\n",
            "C      0.463484    4.668266   -0.727648\n",
            "C     -0.604360    4.681966    0.094755\n",
            "C     -1.154122    3.787730    0.558536\n",
            "C     -0.366400    2.661160    0.509772\n",
            "N     -0.545713    1.527453    1.405867\n",
            "C     -1.654439    0.588139    1.096861\n",
            "C     -0.946350   -0.730985    0.986765\n",
            "C     -1.458703   -1.978537    0.631927\n",
            "C     -0.568305   -3.055676    0.531479\n",
            "C      0.796344   -2.897066    0.805788\n",
            "C      1.281284   -1.675899    1.303051\n",
            "C      0.388355   -0.588972    1.364326\n",
            "C      0.618666    0.779734    1.893732\n",
            "H      1.576950    1.199467    1.544932\n",
            "H      0.643490    0.845607    2.998897\n",
            "H      2.331038   -1.571928    1.569631\n",
            "C      1.819239   -4.153980    0.582861\n",
            "H      1.748378   -4.491263   -0.466399\n",
            "H      2.864613   -3.813357    0.788167\n",
            "H      1.608738   -5.077060    1.300036\n",
            "C     -1.131336   -4.546452    0.135986\n",
            "H     -2.263865   -4.542489    0.116333\n",
            "H     -0.747071   -4.803085   -0.865066\n",
            "H     -0.834758   -5.411119    0.888468\n",
            "H     -2.504158   -2.112603    0.380071\n",
            "H     -2.399438    0.628410    1.912198\n",
            "H     -2.189402    0.859660    0.198649\n",
            "H     -2.262904    3.628494    0.667647\n",
            "H     -1.236431    5.734537    0.116591\n",
            "H      1.096271    5.793952   -0.761549\n",
            "H      2.227308    3.719513   -1.230386\n",
            "H      0.831163    1.752877   -3.004908\n",
            "H      1.701528    0.239469   -2.573188\n",
            "H     -1.284137    0.740550   -2.132222\n",
            "H     -0.422451   -0.775361   -1.754622\n",
            "H     -0.599008   -0.255032   -3.445454\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3R4QBQeKttN"
      },
      "source": [
        "This 2D rendering is the equivalent of the **input to the model**!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "gkQRWjraaKex",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        },
        "outputId": "b8a79f9f-24c2-4da9-eb2c-eb9ec8b892eb"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.SVG object>"
            ],
            "image/svg+xml": "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:rdkit=\"http://www.rdkit.org/xml\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" version=\"1.1\" baseProfile=\"full\" xml:space=\"preserve\" width=\"450px\" height=\"300px\" viewBox=\"0 0 450 300\">\n<!-- END OF HEADER -->\n<rect style=\"opacity:1.0;fill:#FFFFFF;stroke:none\" width=\"450.0\" height=\"300.0\" x=\"0.0\" y=\"0.0\"> </rect>\n<path class=\"bond-0 atom-0 atom-1\" d=\"M 348.5,23.0 L 368.3,75.7\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n<path class=\"bond-1 atom-1 atom-2\" d=\"M 368.3,75.7 L 354.5,92.4\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n<path class=\"bond-1 atom-1 atom-2\" d=\"M 354.5,92.4 L 340.7,109.2\" style=\"fill:none;fill-rule:evenodd;stroke:#FF0000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n<path class=\"bond-2 atom-2 atom-3\" d=\"M 336.4,129.2 L 344.3,150.5\" style=\"fill:none;fill-rule:evenodd;stroke:#FF0000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n<path class=\"bond-2 atom-2 atom-3\" d=\"M 344.3,150.5 L 352.3,171.7\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n<path class=\"bond-3 atom-3 atom-4\" d=\"M 352.3,171.7 L 407.8,180.9\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n<path class=\"bond-3 atom-3 atom-4\" d=\"M 355.7,180.8 L 401.6,188.4\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n<path class=\"bond-4 atom-4 atom-5\" d=\"M 407.8,180.9 L 427.5,233.6\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n<path class=\"bond-5 atom-5 atom-6\" d=\"M 427.5,233.6 L 391.8,277.0\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n<path class=\"bond-5 atom-5 atom-6\" d=\"M 417.9,232.0 L 388.4,267.9\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n<path class=\"bond-6 atom-6 atom-7\" d=\"M 391.8,277.0 L 336.3,267.8\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n<path class=\"bond-7 atom-7 atom-8\" d=\"M 336.3,267.8 L 316.6,215.1\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n<path class=\"bond-7 atom-7 atom-8\" d=\"M 342.5,260.2 L 326.2,216.7\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n<path class=\"bond-8 atom-8 atom-9\" d=\"M 316.6,215.1 L 292.8,211.2\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n<path class=\"bond-8 atom-8 atom-9\" d=\"M 292.8,211.2 L 269.0,207.2\" style=\"fill:none;fill-rule:evenodd;stroke:#0000FF;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n<path class=\"bond-9 atom-9 atom-10\" d=\"M 253.2,213.7 L 237.2,229.5\" style=\"fill:none;fill-rule:evenodd;stroke:#0000FF;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n<path class=\"bond-9 atom-9 atom-10\" d=\"M 237.2,229.5 L 221.1,245.4\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n<path class=\"bond-10 atom-10 atom-11\" d=\"M 221.1,245.4 L 171.2,219.5\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n<path class=\"bond-11 atom-11 atom-12\" d=\"M 171.2,219.5 L 118.6,239.2\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n<path class=\"bond-11 atom-11 atom-12\" d=\"M 163.7,213.3 L 120.1,229.6\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n<path class=\"bond-12 atom-12 atom-13\" d=\"M 118.6,239.2 L 75.1,203.5\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n<path class=\"bond-13 atom-13 atom-14\" d=\"M 75.1,203.5 L 22.5,223.3\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n<path class=\"bond-14 atom-13 atom-15\" d=\"M 75.1,203.5 L 84.3,148.1\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n<path class=\"bond-14 atom-13 atom-15\" d=\"M 84.2,200.1 L 91.9,154.2\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n<path class=\"bond-15 atom-15 atom-16\" d=\"M 84.3,148.1 L 40.9,112.3\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n<path class=\"bond-16 atom-15 atom-17\" d=\"M 84.3,148.1 L 137.0,128.3\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n<path class=\"bond-17 atom-17 atom-18\" d=\"M 137.0,128.3 L 180.4,164.0\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n<path class=\"bond-17 atom-17 atom-18\" d=\"M 135.4,137.9 L 171.3,167.4\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n<path class=\"bond-18 atom-18 atom-19\" d=\"M 180.4,164.0 L 236.0,155.6\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n<path class=\"bond-19 atom-8 atom-3\" d=\"M 316.6,215.1 L 352.3,171.7\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n<path class=\"bond-20 atom-19 atom-9\" d=\"M 236.0,155.6 L 246.1,175.9\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n<path class=\"bond-20 atom-19 atom-9\" d=\"M 246.1,175.9 L 256.2,196.1\" style=\"fill:none;fill-rule:evenodd;stroke:#0000FF;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n<path class=\"bond-21 atom-18 atom-11\" d=\"M 180.4,164.0 L 171.2,219.5\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n<path d=\"M 367.3,73.0 L 368.3,75.7 L 367.6,76.5\" style=\"fill:none;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:10;stroke-opacity:1;\"/>\n<path d=\"M 405.0,180.5 L 407.8,180.9 L 408.7,183.6\" style=\"fill:none;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:10;stroke-opacity:1;\"/>\n<path d=\"M 426.5,230.9 L 427.5,233.6 L 425.7,235.7\" style=\"fill:none;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:10;stroke-opacity:1;\"/>\n<path d=\"M 393.6,274.8 L 391.8,277.0 L 389.0,276.5\" style=\"fill:none;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:10;stroke-opacity:1;\"/>\n<path d=\"M 339.1,268.2 L 336.3,267.8 L 335.3,265.1\" style=\"fill:none;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:10;stroke-opacity:1;\"/>\n<path d=\"M 221.9,244.6 L 221.1,245.4 L 218.6,244.1\" style=\"fill:none;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:10;stroke-opacity:1;\"/>\n<path d=\"M 121.2,238.2 L 118.6,239.2 L 116.4,237.4\" style=\"fill:none;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:10;stroke-opacity:1;\"/>\n<path d=\"M 134.3,129.3 L 137.0,128.3 L 139.1,130.1\" style=\"fill:none;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:10;stroke-opacity:1;\"/>\n<path d=\"M 233.2,156.1 L 236.0,155.6 L 236.5,156.6\" style=\"fill:none;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:10;stroke-opacity:1;\"/>\n<path class=\"atom-2\" d=\"M 325.2 119.1 Q 325.2 115.3, 327.1 113.2 Q 329.0 111.0, 332.5 111.0 Q 336.1 111.0, 338.0 113.2 Q 339.9 115.3, 339.9 119.1 Q 339.9 123.0, 337.9 125.2 Q 336.0 127.4, 332.5 127.4 Q 329.0 127.4, 327.1 125.2 Q 325.2 123.0, 325.2 119.1 M 332.5 125.6 Q 335.0 125.6, 336.3 124.0 Q 337.6 122.3, 337.6 119.1 Q 337.6 116.0, 336.3 114.4 Q 335.0 112.8, 332.5 112.8 Q 330.1 112.8, 328.8 114.4 Q 327.5 116.0, 327.5 119.1 Q 327.5 122.3, 328.8 124.0 Q 330.1 125.6, 332.5 125.6 \" fill=\"#FF0000\"/>\n<path class=\"atom-9\" d=\"M 257.6 198.0 L 262.8 206.4 Q 263.3 207.2, 264.2 208.7 Q 265.0 210.2, 265.1 210.3 L 265.1 198.0 L 267.2 198.0 L 267.2 213.9 L 265.0 213.9 L 259.4 204.7 Q 258.7 203.6, 258.0 202.3 Q 257.4 201.1, 257.2 200.7 L 257.2 213.9 L 255.1 213.9 L 255.1 198.0 L 257.6 198.0 \" fill=\"#0000FF\"/>\n</svg>"
          },
          "metadata": {}
        }
      ],
      "source": [
        "mc = Chem.MolFromSmiles(dataset[idx]['smiles'])\n",
        "molSize=(450,300)\n",
        "drawer = MD2.MolDraw2DSVG(molSize[0],molSize[1])\n",
        "drawer.DrawMolecule(mc)\n",
        "drawer.FinishDrawing()\n",
        "svg = drawer.GetDrawingText()\n",
        "display(SVG(svg.replace('svg:','')))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4FDMYMxKw2I"
      },
      "source": [
        "Generate the 3d molecule!"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q py3Dmol"
      ],
      "metadata": {
        "id": "SBgIxb_BtK_N"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import py3Dmol\n",
        "xyz = Chem.MolToXYZBlock(mols_gen[idx])\n",
        "\n",
        "view = py3Dmol.view(width=400, height=400)\n",
        "view.addModel(xyz, 'xyz')\n",
        "view.setStyle({'stick': {}})\n",
        "view.zoomTo()\n",
        "view.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "id": "tUH5h11atLrV",
        "outputId": "bbc62786-99d3-4e57-8bc7-95a85c54e970"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/3dmoljs_load.v0": "<div id=\"3dmolviewer_1755667775297204\"  style=\"position: relative; width: 400px; height: 400px;\">\n        <p id=\"3dmolwarning_1755667775297204\" style=\"background-color:#ffcccc;color:black\">3Dmol.js failed to load for some reason.  Please check your browser console for error messages.<br></p>\n        </div>\n<script>\n\nvar loadScriptAsync = function(uri){\n  return new Promise((resolve, reject) => {\n    //this is to ignore the existence of requirejs amd\n    var savedexports, savedmodule;\n    if (typeof exports !== 'undefined') savedexports = exports;\n    else exports = {}\n    if (typeof module !== 'undefined') savedmodule = module;\n    else module = {}\n\n    var tag = document.createElement('script');\n    tag.src = uri;\n    tag.async = true;\n    tag.onload = () => {\n        exports = savedexports;\n        module = savedmodule;\n        resolve();\n    };\n  var firstScriptTag = document.getElementsByTagName('script')[0];\n  firstScriptTag.parentNode.insertBefore(tag, firstScriptTag);\n});\n};\n\nif(typeof $3Dmolpromise === 'undefined') {\n$3Dmolpromise = null;\n  $3Dmolpromise = loadScriptAsync('https://cdn.jsdelivr.net/npm/3dmol@2.5.2/build/3Dmol-min.js');\n}\n\nvar viewer_1755667775297204 = null;\nvar warn = document.getElementById(\"3dmolwarning_1755667775297204\");\nif(warn) {\n    warn.parentNode.removeChild(warn);\n}\n$3Dmolpromise.then(function() {\nviewer_1755667775297204 = $3Dmol.createViewer(document.getElementById(\"3dmolviewer_1755667775297204\"),{backgroundColor:\"white\"});\nviewer_1755667775297204.zoomTo();\n\tviewer_1755667775297204.addModel(\"41\\n\\nC     -0.447951    0.105039   -2.442769\\nC      0.854655    0.921963   -2.340018\\nO      1.083520    1.336908   -0.978605\\nC      0.590669    2.620887   -0.604237\\nC      1.095609    3.659087   -1.056251\\nC      0.463484    4.668266   -0.727648\\nC     -0.604360    4.681966    0.094755\\nC     -1.154122    3.787730    0.558536\\nC     -0.366400    2.661160    0.509772\\nN     -0.545713    1.527453    1.405867\\nC     -1.654439    0.588139    1.096861\\nC     -0.946350   -0.730985    0.986765\\nC     -1.458703   -1.978537    0.631927\\nC     -0.568305   -3.055676    0.531479\\nC      0.796344   -2.897066    0.805788\\nC      1.281284   -1.675899    1.303051\\nC      0.388355   -0.588972    1.364326\\nC      0.618666    0.779734    1.893732\\nH      1.576950    1.199467    1.544932\\nH      0.643490    0.845607    2.998897\\nH      2.331038   -1.571928    1.569631\\nC      1.819239   -4.153980    0.582861\\nH      1.748378   -4.491263   -0.466399\\nH      2.864613   -3.813357    0.788167\\nH      1.608738   -5.077060    1.300036\\nC     -1.131336   -4.546452    0.135986\\nH     -2.263865   -4.542489    0.116333\\nH     -0.747071   -4.803085   -0.865066\\nH     -0.834758   -5.411119    0.888468\\nH     -2.504158   -2.112603    0.380071\\nH     -2.399438    0.628410    1.912198\\nH     -2.189402    0.859660    0.198649\\nH     -2.262904    3.628494    0.667647\\nH     -1.236431    5.734537    0.116591\\nH      1.096271    5.793952   -0.761549\\nH      2.227308    3.719513   -1.230386\\nH      0.831163    1.752877   -3.004908\\nH      1.701528    0.239469   -2.573188\\nH     -1.284137    0.740550   -2.132222\\nH     -0.422451   -0.775361   -1.754622\\nH     -0.599008   -0.255032   -3.445454\\n\",\"xyz\");\n\tviewer_1755667775297204.setStyle({\"stick\": {}});\n\tviewer_1755667775297204.zoomTo();\nviewer_1755667775297204.render();\n});\n</script>",
            "text/html": [
              "<div id=\"3dmolviewer_1755667775297204\"  style=\"position: relative; width: 400px; height: 400px;\">\n",
              "        <p id=\"3dmolwarning_1755667775297204\" style=\"background-color:#ffcccc;color:black\">3Dmol.js failed to load for some reason.  Please check your browser console for error messages.<br></p>\n",
              "        </div>\n",
              "<script>\n",
              "\n",
              "var loadScriptAsync = function(uri){\n",
              "  return new Promise((resolve, reject) => {\n",
              "    //this is to ignore the existence of requirejs amd\n",
              "    var savedexports, savedmodule;\n",
              "    if (typeof exports !== 'undefined') savedexports = exports;\n",
              "    else exports = {}\n",
              "    if (typeof module !== 'undefined') savedmodule = module;\n",
              "    else module = {}\n",
              "\n",
              "    var tag = document.createElement('script');\n",
              "    tag.src = uri;\n",
              "    tag.async = true;\n",
              "    tag.onload = () => {\n",
              "        exports = savedexports;\n",
              "        module = savedmodule;\n",
              "        resolve();\n",
              "    };\n",
              "  var firstScriptTag = document.getElementsByTagName('script')[0];\n",
              "  firstScriptTag.parentNode.insertBefore(tag, firstScriptTag);\n",
              "});\n",
              "};\n",
              "\n",
              "if(typeof $3Dmolpromise === 'undefined') {\n",
              "$3Dmolpromise = null;\n",
              "  $3Dmolpromise = loadScriptAsync('https://cdn.jsdelivr.net/npm/3dmol@2.5.2/build/3Dmol-min.js');\n",
              "}\n",
              "\n",
              "var viewer_1755667775297204 = null;\n",
              "var warn = document.getElementById(\"3dmolwarning_1755667775297204\");\n",
              "if(warn) {\n",
              "    warn.parentNode.removeChild(warn);\n",
              "}\n",
              "$3Dmolpromise.then(function() {\n",
              "viewer_1755667775297204 = $3Dmol.createViewer(document.getElementById(\"3dmolviewer_1755667775297204\"),{backgroundColor:\"white\"});\n",
              "viewer_1755667775297204.zoomTo();\n",
              "\tviewer_1755667775297204.addModel(\"41\\n\\nC     -0.447951    0.105039   -2.442769\\nC      0.854655    0.921963   -2.340018\\nO      1.083520    1.336908   -0.978605\\nC      0.590669    2.620887   -0.604237\\nC      1.095609    3.659087   -1.056251\\nC      0.463484    4.668266   -0.727648\\nC     -0.604360    4.681966    0.094755\\nC     -1.154122    3.787730    0.558536\\nC     -0.366400    2.661160    0.509772\\nN     -0.545713    1.527453    1.405867\\nC     -1.654439    0.588139    1.096861\\nC     -0.946350   -0.730985    0.986765\\nC     -1.458703   -1.978537    0.631927\\nC     -0.568305   -3.055676    0.531479\\nC      0.796344   -2.897066    0.805788\\nC      1.281284   -1.675899    1.303051\\nC      0.388355   -0.588972    1.364326\\nC      0.618666    0.779734    1.893732\\nH      1.576950    1.199467    1.544932\\nH      0.643490    0.845607    2.998897\\nH      2.331038   -1.571928    1.569631\\nC      1.819239   -4.153980    0.582861\\nH      1.748378   -4.491263   -0.466399\\nH      2.864613   -3.813357    0.788167\\nH      1.608738   -5.077060    1.300036\\nC     -1.131336   -4.546452    0.135986\\nH     -2.263865   -4.542489    0.116333\\nH     -0.747071   -4.803085   -0.865066\\nH     -0.834758   -5.411119    0.888468\\nH     -2.504158   -2.112603    0.380071\\nH     -2.399438    0.628410    1.912198\\nH     -2.189402    0.859660    0.198649\\nH     -2.262904    3.628494    0.667647\\nH     -1.236431    5.734537    0.116591\\nH      1.096271    5.793952   -0.761549\\nH      2.227308    3.719513   -1.230386\\nH      0.831163    1.752877   -3.004908\\nH      1.701528    0.239469   -2.573188\\nH     -1.284137    0.740550   -2.132222\\nH     -0.422451   -0.775361   -1.754622\\nH     -0.599008   -0.255032   -3.445454\\n\",\"xyz\");\n",
              "\tviewer_1755667775297204.setStyle({\"stick\": {}});\n",
              "\tviewer_1755667775297204.zoomTo();\n",
              "viewer_1755667775297204.render();\n",
              "});\n",
              "</script>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **üßêDenoising trajectory visualization**"
      ],
      "metadata": {
        "id": "DCXUFiLMhfb4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = dataset[idx]\n",
        "\n",
        "num_samples = max(data.pos_ref.size(0) // data.num_nodes, 1)\n",
        "data_input = data.clone()\n",
        "data_input['pos_ref'] = None\n",
        "batch = repeat_data(data_input, num_samples).to(DEVICE)\n",
        "\n",
        "# save more intermediate timesteps\n",
        "save_steps = [999, 950, 900, 850, 800, 750, 700, 600, 650, 500, 400, 300, 200, 150, 100, 75, 50, 30, 20, 10, 5, 0]\n",
        "pos_trajectory = {}\n",
        "noise_levels = {}\n",
        "mol_trajectory = {}\n",
        "\n",
        "# initial configuration\n",
        "pos_init = torch.randn(batch.num_nodes, 3).to(DEVICE)\n",
        "\n",
        "with torch.no_grad():\n",
        "    # scale initial sample\n",
        "    pos = pos_init * sigmas[-1]\n",
        "\n",
        "    for t in scheduler.timesteps:\n",
        "        batch.pos = pos\n",
        "\n",
        "        # generate geometry with model\n",
        "        epsilon = model.forward(batch, t, sigma=sigmas[t], return_dict=False)[0]\n",
        "\n",
        "        # Update\n",
        "        reconstructed_pos = scheduler.step(epsilon, t, pos)[\"prev_sample\"].to(DEVICE)\n",
        "\n",
        "        pos = reconstructed_pos\n",
        "\n",
        "        if torch.isnan(pos).any():\n",
        "            print(\"NaN detected. Please restart.\")\n",
        "            raise FloatingPointError()\n",
        "\n",
        "        # recenter graph of positions\n",
        "        pos = pos - scatter_mean(pos, batch.batch, dim=0)[batch.batch]\n",
        "\n",
        "        # optional clipping\n",
        "        if clip_pos is not None:\n",
        "            pos = torch.clamp(pos, min=-clip_pos, max=clip_pos)\n",
        "\n",
        "        # save the chosen timestep\n",
        "        if t.item() in save_steps:\n",
        "            pos_trajectory[t.item()] = pos.clone().cpu()\n",
        "            noise_levels[t.item()] = sigmas[t].item()\n",
        "\n",
        "            mol_at_t = set_rdmol_positions(data.rdmol, pos.clone().cpu().numpy())\n",
        "            mol_trajectory[t.item()] = mol_at_t\n",
        "\n",
        "print(f\"number of saved time steps: {len(pos_trajectory)}\")\n",
        "\n",
        "pos_gen = pos.cpu()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0mvxbGO-gwKs",
        "outputId": "32b7eb5f-f300-4df3-ce6f-9a9dd6566c18"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of saved time steps: 22\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ipywidgets import interact\n",
        "\n",
        "@interact(step_idx=(0, len(mol_trajectory)-1, 1))\n",
        "def show_denoising_step_rdkit(step_idx=0):\n",
        "    all_steps = sorted(mol_trajectory.keys(), reverse=True)\n",
        "    step = all_steps[step_idx]\n",
        "\n",
        "    # RDKit\n",
        "    mol = mol_trajectory[step]\n",
        "    xyz_block = Chem.MolToXYZBlock(mol)\n",
        "\n",
        "    view = py3Dmol.view(width=600, height=500)\n",
        "    view.addModel(xyz_block, 'xyz')\n",
        "    view.setStyle({'stick': {'radius': 0.15}, 'sphere': {'radius': 0.25}})\n",
        "    view.setBackgroundColor('white')\n",
        "    view.zoomTo()\n",
        "\n",
        "    print(f\"Timestep: {step}, Noise level: œÉ = {noise_levels.get(step, 0):.3f}\")\n",
        "    view.show()"
      ],
      "metadata": {
        "id": "rX34ejJBjiRY"
      },
      "execution_count": 62,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}